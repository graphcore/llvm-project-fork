; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --remove_checks
; RUN: llc < %s -march=colossus -colossus-coissue=false -mattr=+ipu1 | FileCheck %s
; RUN: llc < %s -march=colossus -colossus-coissue=false -mattr=+ipu2 | FileCheck %s

target triple = "colossus-graphcore--elf"

; f16 <-> i64 are custom legalised to go via i32, instead of via f32
define half @csitofp_i64_to_f16() {
; CHECK-LABEL: csitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 29191
; CHECK-NEXT:    br $m10
  %retval = sitofp i64 12345 to half
  ret half %retval
}

declare half @llvm.experimental.constrained.sitofp.f16.i64(i64, metadata, metadata)

; Constant folding does not occur with strict FP.
define half @constrained_csitofp_i64_to_f16() {
; CHECK-LABEL: constrained_csitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 12345
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    br $m10
  %retval = call half @llvm.experimental.constrained.sitofp.f16.i64(i64 12345, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret half %retval
}

; sign extension induced by the abi
define half @sitofp_i64_to_f16(i64 signext %src) {
; CHECK-LABEL: sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = sitofp i64 %src to half
  ret half %retval
}

; sign extension induced by the abi
define half @constrained_sitofp_i64_to_f16(i64 signext %src) {
; CHECK-LABEL: constrained_sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call half @llvm.experimental.constrained.sitofp.f16.i64(i64 %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret half %retval
}

; Sign extension induced by the ABI
define <1 x half> @v1sitofp_i64_to_f16(<1 x i64> %src) {
; CHECK-LABEL: v1sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = sitofp <1 x i64> %src to <1 x half>
  ret <1 x half> %retval
}

declare <1 x half> @llvm.experimental.constrained.sitofp.v1f16.v1i64(<1 x i64>, metadata, metadata)

; Sign extension induced by the ABI
define <1 x half> @constrained_v1sitofp_i64_to_f16(<1 x i64> %src) {
; CHECK-LABEL: constrained_v1sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call <1 x half> @llvm.experimental.constrained.sitofp.v1f16.v1i64(<1 x i64> %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret <1 x half> %retval
}

; Sign extension induced by the ABI
define <2 x half> @v2sitofp_i64_to_f16(<2 x i64> %src) {
; CHECK-LABEL: v2sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    st32 $m2, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a1, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a1, $a1
; CHECK-NEXT:    f32tof16 $a1, $a1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    sort4x16lo $a0, $a0, $a1
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = sitofp <2 x i64> %src to <2 x half>
  ret <2 x half> %retval
}

declare <2 x half> @llvm.experimental.constrained.sitofp.v2f16.v2i64(<2 x i64>, metadata, metadata)

; Sign extension induced by the ABI
define <2 x half> @constrained_v2sitofp_i64_to_f16(<2 x i64> %src) {
; CHECK-LABEL: constrained_v2sitofp_i64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    st32 $m2, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a1, $m11, $m15, 1
; CHECK-NEXT:    f32fromi32 $a1, $a1
; CHECK-NEXT:    f32tof16 $a1, $a1
; CHECK-NEXT:    f32fromi32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    sort4x16lo $a0, $a0, $a1
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call <2 x half> @llvm.experimental.constrained.sitofp.v2f16.v2i64(<2 x i64> %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret <2 x half> %retval
}

define half @cuitofp_u64_to_f16() {
; CHECK-LABEL: cuitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 29191
; CHECK-NEXT:    br $m10
  %retval = uitofp i64 12345 to half
  ret half %retval
}

declare half @llvm.experimental.constrained.uitofp.f16.i64(i64, metadata, metadata)

define half @constrained_cuitofp_u64_to_f16() {
; CHECK-LABEL: constrained_cuitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 12345
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    br $m10
  %retval = call half @llvm.experimental.constrained.uitofp.f16.i64(i64 12345, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret half %retval
}

; Zero extension induced by the ABI
define half @uitofp_u64_to_f16(i64 zeroext %src) {
; CHECK-LABEL: uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = uitofp i64 %src to half
  ret half %retval
}

; Zero extension induced by the ABI
define half @constrained_uitofp_u64_to_f16(i64 zeroext %src) {
; CHECK-LABEL: constrained_uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call half @llvm.experimental.constrained.uitofp.f16.i64(i64 %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret half %retval
}

; Zero extension induced by the ABI
define <1 x half> @v1uitofp_u64_to_f16(<1 x i64> %src) {
; CHECK-LABEL: v1uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = uitofp <1 x i64> %src to <1 x half>
  ret <1 x half> %retval
}

declare <1 x half> @llvm.experimental.constrained.uitofp.v1f16.v1i64(<1 x i64>, metadata, metadata)

; Zero extension induced by the ABI
define <1 x half> @constrained_v1uitofp_u64_to_f16(<1 x i64> %src) {
; CHECK-LABEL: constrained_v1uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call <1 x half> @llvm.experimental.constrained.uitofp.v1f16.v1i64(<1 x i64> %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret <1 x half> %retval
}

; Zero extension induced by the ABI
define <2 x half> @v2uitofp_u64_to_f16(<2 x i64> %src) {
; CHECK-LABEL: v2uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    st32 $m2, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a1, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a1, $a1
; CHECK-NEXT:    f32tof16 $a1, $a1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    sort4x16lo $a0, $a0, $a1
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = uitofp <2 x i64> %src to <2 x half>
  ret <2 x half> %retval
}

declare <2 x half> @llvm.experimental.constrained.uitofp.v2f16.v2i64(<2 x i64>, metadata, metadata)

; Zero extension induced by the ABI
define <2 x half> @constrained_v2uitofp_u64_to_f16(<2 x i64> %src) {
; CHECK-LABEL: constrained_v2uitofp_u64_to_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    add $m11, $m11, -8
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    st32 $m0, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a0, $m11, $m15, 1
; CHECK-NEXT:    st32 $m2, $m11, $m15, 1
; CHECK-NEXT:    ld32 $a1, $m11, $m15, 1
; CHECK-NEXT:    f32fromui32 $a1, $a1
; CHECK-NEXT:    f32tof16 $a1, $a1
; CHECK-NEXT:    f32fromui32 $a0, $a0
; CHECK-NEXT:    f32tof16 $a0, $a0
; CHECK-NEXT:    sort4x16lo $a0, $a0, $a1
; CHECK-NEXT:    add $m11, $m11, 8
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    br $m10
  %retval = call <2 x half> @llvm.experimental.constrained.uitofp.v2f16.v2i64(<2 x i64> %src, metadata !"round.tonearest", metadata !"fpexcept.strict")
  ret <2 x half> %retval
}

define signext i64 @cfptosi_f16_to_i64() {
; CHECK-LABEL: cfptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 65244
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = fptosi half 0xHFEDC to i64
  ret i64 %retval
}

declare i64 @llvm.experimental.constrained.fptosi.i64.f16(half, metadata)

define signext i64 @constrained_cfptosi_f16_to_i64() {
; CHECK-LABEL: constrained_cfptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    setzi $a0, 65244
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = call i64 @llvm.experimental.constrained.fptosi.i64.f16(half 0xHFEDC, metadata !"fpexcept.strict")
  ret i64 %retval
}

define signext i64 @fptosi_f16_to_i64(half %src) {
; CHECK-LABEL: fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = fptosi half %src to i64
  ret i64 %retval
}

define signext i64 @constrained_fptosi_f16_to_i64(half %src) {
; CHECK-LABEL: constrained_fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = call i64 @llvm.experimental.constrained.fptosi.i64.f16(half %src, metadata !"fpexcept.strict")
  ret i64 %retval
}

define <1 x i64> @v1fptosi_f16_to_i64(<1 x half> %src) {
; CHECK-LABEL: v1fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = fptosi <1 x half> %src to <1 x i64>
  ret <1 x i64> %retval
}

declare <1 x i64> @llvm.experimental.constrained.fptosi.v1i64.v1f16(<1 x half>, metadata)

define <1 x i64> @constrained_v1fptosi_f16_to_i64(<1 x half> %src) {
; CHECK-LABEL: constrained_v1fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    br $m10
  %retval = call <1 x i64> @llvm.experimental.constrained.fptosi.v1i64.v1f16(<1 x half> %src, metadata !"fpexcept.strict")
  ret <1 x i64> %retval
}

define <2 x i64> @v2fptosi_f16_to_i64(<2 x half> %src) {
; CHECK-LABEL: v2fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    swap16 $a1, $a0
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    f16tof32 $a0, $a1
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m2, $a0
; CHECK-NEXT:    shrs $m3, $m2, 31
; CHECK-NEXT:    br $m10
  %retval = fptosi <2 x half> %src to <2 x i64>
  ret <2 x i64> %retval
}

declare <2 x i64> @llvm.experimental.constrained.fptosi.v2i64.v2f16(<2 x half>, metadata)

define <2 x i64> @constrained_v2fptosi_f16_to_i64(<2 x half> %src) {
; CHECK-LABEL: constrained_v2fptosi_f16_to_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    swap16 $a1, $a0
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    shrs $m1, $m0, 31
; CHECK-NEXT:    f16tof32 $a0, $a1
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toi32 $a0, $a0
; CHECK-NEXT:    mov $m2, $a0
; CHECK-NEXT:    shrs $m3, $m2, 31
; CHECK-NEXT:    br $m10
  %retval = call <2 x i64> @llvm.experimental.constrained.fptosi.v2i64.v2f16(<2 x half> %src, metadata !"fpexcept.strict")
  ret <2 x i64> %retval
}

define zeroext i64 @cfptoui_f16_to_u64() {
; CHECK-LABEL: cfptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    setzi $a0, 65244
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = fptoui half 0xHFEDC to i64
  ret i64 %retval
}

declare i64 @llvm.experimental.constrained.fptoui.i64.f16(half, metadata)

define zeroext i64 @constrained_cfptoui_f16_to_u64() {
; CHECK-LABEL: constrained_cfptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    setzi $a0, 65244
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = call i64 @llvm.experimental.constrained.fptoui.i64.f16(half 0xHFEDC, metadata !"fpexcept.strict")
  ret i64 %retval
}

define zeroext i64 @fptoui_f16_to_u64(half %src) {
; CHECK-LABEL: fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = fptoui half %src to i64
  ret i64 %retval
}

define zeroext i64 @constrained_fptoui_f16_to_u64(half %src) {
; CHECK-LABEL: constrained_fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = call i64 @llvm.experimental.constrained.fptoui.i64.f16(half %src, metadata !"fpexcept.strict")
  ret i64 %retval
}

define <1 x i64> @v1fptoui_f16_to_u64(<1 x half> %src) {
; CHECK-LABEL: v1fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = fptoui <1 x half> %src to <1 x i64>
  ret <1 x i64> %retval
}

declare <1 x i64> @llvm.experimental.constrained.fptoui.v1i64.v1f16(<1 x half>, metadata)

define <1 x i64> @constrained_v1fptoui_f16_to_u64(<1 x half> %src) {
; CHECK-LABEL: constrained_v1fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    br $m10
  %retval = call <1 x i64> @llvm.experimental.constrained.fptoui.v1i64.v1f16(<1 x half> %src, metadata !"fpexcept.strict")
  ret <1 x i64> %retval
}

define <2 x i64> @v2fptoui_f16_to_u64(<2 x half> %src) {
; CHECK-LABEL: v2fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    mov $m3, $m15
; CHECK-NEXT:    swap16 $a1, $a0
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    f16tof32 $a0, $a1
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m2, $a0
; CHECK-NEXT:    br $m10
  %retval = fptoui <2 x half> %src to <2 x i64>
  ret <2 x i64> %retval
}

declare <2 x i64> @llvm.experimental.constrained.fptoui.v2i64.v2f16(<2 x half>, metadata)

define <2 x i64> @constrained_v2fptoui_f16_to_u64(<2 x half> %src) {
; CHECK-LABEL: constrained_v2fptoui_f16_to_u64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mov $m1, $m15
; CHECK-NEXT:    mov $m3, $m15
; CHECK-NEXT:    swap16 $a1, $a0
; CHECK-NEXT:    f16tof32 $a0, $a0
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m0, $a0
; CHECK-NEXT:    f16tof32 $a0, $a1
; CHECK-NEXT:    f32int $a0, $a0, 3
; CHECK-NEXT:    f32toui32 $a0, $a0
; CHECK-NEXT:    mov $m2, $a0
; CHECK-NEXT:    br $m10
  %retval = call <2 x i64> @llvm.experimental.constrained.fptoui.v2i64.v2f16(<2 x half> %src, metadata !"fpexcept.strict")
  ret <2 x i64> %retval
}
